{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch normalization\n",
    "from tensorflow import keras\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, activation='elu', kernel_initializer='he_normal'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, activation='elu', kernel_initializer='he_normal'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 784)               3136      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                3010      \n",
      "=================================================================\n",
      "Total params: 334,346\n",
      "Trainable params: 331,578\n",
      "Non-trainable params: 2,768\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('batch_normalization/gamma:0', True),\n",
       " ('batch_normalization/beta:0', True),\n",
       " ('batch_normalization/moving_mean:0', False),\n",
       " ('batch_normalization/moving_variance:0', False)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(var.name, var.trainable) for var in model.layers[1].variables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Operation 'cond/Identity' type=Identity>,\n",
       " <tf.Operation 'cond_1/Identity' type=Identity>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[1].updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch normalization before activation\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28,28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, kernel_initializer='he_normal', use_bias=False),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation('elu'),\n",
    "    keras.layers.Dense(100, kernel_initializer='he_normal', use_bias=False),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation('elu'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient clipping (used in RNNs due to hard implementation of batch normalization)\n",
    "optimizer = keras.optimizers.SGD(clipvalue=1.0, clipnorm=1.0)\n",
    "model.compile(loss='mse', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer learning w/ Keras\n",
    "import numpy as np\n",
    "\n",
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "X_train_full = X_train_full / 255.0\n",
    "X_test = X_test / 255.0\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "\n",
    "\n",
    "def split_dataset(X, y):\n",
    "    y_5_or_6 = (y == 5) | (y == 6) # sandals or shirts\n",
    "    y_A = y[~y_5_or_6]\n",
    "    y_A[y_A > 6] -= 2 # class indices 7, 8, 9 should be moved to 5, 6, 7\n",
    "    y_B = (y[y_5_or_6] == 6).astype(np.float32) # binary classification task: is it a shirt (class 6)?\n",
    "    return ((X[~y_5_or_6], y_A),\n",
    "            (X[y_5_or_6], y_B))\n",
    "\n",
    "(X_train_A, y_train_A), (X_train_B, y_train_B) = split_dataset(X_train, y_train)\n",
    "(X_valid_A, y_valid_A), (X_valid_B, y_valid_B) = split_dataset(X_valid, y_valid)\n",
    "(X_test_A, y_test_A), (X_test_B, y_test_B) = split_dataset(X_test, y_test)\n",
    "X_train_B = X_train_B[:200]\n",
    "y_train_B = y_train_B[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((43986, 28, 28),\n",
       " (200, 28, 28),\n",
       " array([4, 0, 5, 7, 7, 7, 4, 4, 3, 4], dtype=uint8),\n",
       " array([1., 1., 0., 0., 0., 0., 1., 1., 1., 0.], dtype=float32))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_A.shape, X_train_B.shape, y_train_A[:10], y_train_B[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A = keras.models.Sequential()\n",
    "model_A.add(keras.layers.Flatten(input_shape=[28,28]))\n",
    "for n_hidden in (300, 100, 50, 50, 50):\n",
    "    model_A.add(keras.layers.Dense(n_hidden, activation='selu'))\n",
    "model_A.add(keras.layers.Dense(8, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A.compile(loss='sparse_categorical_crossentropy',\n",
    "                optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 43986 samples, validate on 4014 samples\n",
      "Epoch 1/20\n",
      "43986/43986 [==============================] - 6s 144us/sample - loss: 0.5799 - accuracy: 0.8160 - val_loss: 0.3762 - val_accuracy: 0.8752\n",
      "Epoch 2/20\n",
      "43986/43986 [==============================] - 5s 119us/sample - loss: 0.3587 - accuracy: 0.8776 - val_loss: 0.3231 - val_accuracy: 0.8906\n",
      "Epoch 3/20\n",
      "43986/43986 [==============================] - 9s 195us/sample - loss: 0.3214 - accuracy: 0.8895 - val_loss: 0.2998 - val_accuracy: 0.8994\n",
      "Epoch 4/20\n",
      "43986/43986 [==============================] - 5s 119us/sample - loss: 0.3015 - accuracy: 0.8965 - val_loss: 0.2850 - val_accuracy: 0.9063\n",
      "Epoch 5/20\n",
      "43986/43986 [==============================] - 5s 107us/sample - loss: 0.2881 - accuracy: 0.9009 - val_loss: 0.2764 - val_accuracy: 0.9071\n",
      "Epoch 6/20\n",
      "43986/43986 [==============================] - 6s 134us/sample - loss: 0.2776 - accuracy: 0.9043 - val_loss: 0.2681 - val_accuracy: 0.9113\n",
      "Epoch 7/20\n",
      "43986/43986 [==============================] - 8s 192us/sample - loss: 0.2695 - accuracy: 0.9079 - val_loss: 0.2697 - val_accuracy: 0.9098\n",
      "Epoch 8/20\n",
      "43986/43986 [==============================] - 10s 232us/sample - loss: 0.2627 - accuracy: 0.9098 - val_loss: 0.2622 - val_accuracy: 0.9128\n",
      "Epoch 9/20\n",
      "43986/43986 [==============================] - 6s 146us/sample - loss: 0.2571 - accuracy: 0.9111 - val_loss: 0.2630 - val_accuracy: 0.9113\n",
      "Epoch 10/20\n",
      "43986/43986 [==============================] - 6s 129us/sample - loss: 0.2522 - accuracy: 0.9131 - val_loss: 0.2577 - val_accuracy: 0.9145\n",
      "Epoch 11/20\n",
      "43986/43986 [==============================] - 7s 167us/sample - loss: 0.2476 - accuracy: 0.9154 - val_loss: 0.2517 - val_accuracy: 0.9150\n",
      "Epoch 12/20\n",
      "43986/43986 [==============================] - 8s 180us/sample - loss: 0.2434 - accuracy: 0.9161 - val_loss: 0.2505 - val_accuracy: 0.9188\n",
      "Epoch 13/20\n",
      "43986/43986 [==============================] - 9s 199us/sample - loss: 0.2399 - accuracy: 0.9172 - val_loss: 0.2462 - val_accuracy: 0.9175\n",
      "Epoch 14/20\n",
      "43986/43986 [==============================] - 11s 249us/sample - loss: 0.2370 - accuracy: 0.9185 - val_loss: 0.2456 - val_accuracy: 0.9165\n",
      "Epoch 15/20\n",
      "43986/43986 [==============================] - 6s 144us/sample - loss: 0.2334 - accuracy: 0.9196 - val_loss: 0.2433 - val_accuracy: 0.9160\n",
      "Epoch 16/20\n",
      "43986/43986 [==============================] - 5s 112us/sample - loss: 0.2303 - accuracy: 0.9214 - val_loss: 0.2419 - val_accuracy: 0.9200\n",
      "Epoch 17/20\n",
      "43986/43986 [==============================] - 13s 284us/sample - loss: 0.2278 - accuracy: 0.9214 - val_loss: 0.2490 - val_accuracy: 0.9143\n",
      "Epoch 18/20\n",
      "43986/43986 [==============================] - 10s 226us/sample - loss: 0.2243 - accuracy: 0.9225 - val_loss: 0.2440 - val_accuracy: 0.9180\n",
      "Epoch 19/20\n",
      "43986/43986 [==============================] - 9s 208us/sample - loss: 0.2225 - accuracy: 0.9237 - val_loss: 0.2375 - val_accuracy: 0.9188\n",
      "Epoch 20/20\n",
      "43986/43986 [==============================] - 9s 210us/sample - loss: 0.2200 - accuracy: 0.9251 - val_loss: 0.2473 - val_accuracy: 0.9128\n"
     ]
    }
   ],
   "source": [
    "history = model_A.fit(X_train_A, y_train_A, epochs=20, validation_data=(X_valid_A, y_valid_A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A.save('my_model_A.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_B = keras.models.Sequential()\n",
    "model_B.add(keras.layers.Flatten(input_shape=[28,28]))\n",
    "for n_hidden in (300, 100, 50, 50, 50):\n",
    "    model_B.add(keras.layers.Dense(n_hidden, activation='selu'))\n",
    "model_B.add(keras.layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200 samples, validate on 986 samples\n",
      "Epoch 1/20\n",
      "200/200 [==============================] - 1s 4ms/sample - loss: 0.5440 - accuracy: 0.7300 - val_loss: 0.4762 - val_accuracy: 0.8144\n",
      "Epoch 2/20\n",
      "200/200 [==============================] - 0s 473us/sample - loss: 0.3857 - accuracy: 0.8700 - val_loss: 0.3733 - val_accuracy: 0.8844\n",
      "Epoch 3/20\n",
      "200/200 [==============================] - 0s 545us/sample - loss: 0.2958 - accuracy: 0.9300 - val_loss: 0.3108 - val_accuracy: 0.9077\n",
      "Epoch 4/20\n",
      "200/200 [==============================] - 0s 483us/sample - loss: 0.2412 - accuracy: 0.9600 - val_loss: 0.2701 - val_accuracy: 0.9178\n",
      "Epoch 5/20\n",
      "200/200 [==============================] - 0s 559us/sample - loss: 0.2047 - accuracy: 0.9700 - val_loss: 0.2406 - val_accuracy: 0.9320\n",
      "Epoch 6/20\n",
      "200/200 [==============================] - 0s 712us/sample - loss: 0.1778 - accuracy: 0.9800 - val_loss: 0.2173 - val_accuracy: 0.9442\n",
      "Epoch 7/20\n",
      "200/200 [==============================] - 0s 533us/sample - loss: 0.1564 - accuracy: 0.9850 - val_loss: 0.1975 - val_accuracy: 0.9503\n",
      "Epoch 8/20\n",
      "200/200 [==============================] - 0s 538us/sample - loss: 0.1390 - accuracy: 0.9900 - val_loss: 0.1819 - val_accuracy: 0.9564\n",
      "Epoch 9/20\n",
      "200/200 [==============================] - 1s 3ms/sample - loss: 0.1249 - accuracy: 0.9900 - val_loss: 0.1691 - val_accuracy: 0.9635\n",
      "Epoch 10/20\n",
      "200/200 [==============================] - 0s 414us/sample - loss: 0.1139 - accuracy: 0.9900 - val_loss: 0.1588 - val_accuracy: 0.9655\n",
      "Epoch 11/20\n",
      "200/200 [==============================] - 0s 448us/sample - loss: 0.1044 - accuracy: 0.9950 - val_loss: 0.1496 - val_accuracy: 0.9665\n",
      "Epoch 12/20\n",
      "200/200 [==============================] - 0s 499us/sample - loss: 0.0964 - accuracy: 0.9950 - val_loss: 0.1421 - val_accuracy: 0.9686\n",
      "Epoch 13/20\n",
      "200/200 [==============================] - 0s 434us/sample - loss: 0.0894 - accuracy: 1.0000 - val_loss: 0.1358 - val_accuracy: 0.9686\n",
      "Epoch 14/20\n",
      "200/200 [==============================] - 0s 398us/sample - loss: 0.0837 - accuracy: 1.0000 - val_loss: 0.1303 - val_accuracy: 0.9696\n",
      "Epoch 15/20\n",
      "200/200 [==============================] - 0s 420us/sample - loss: 0.0786 - accuracy: 1.0000 - val_loss: 0.1251 - val_accuracy: 0.9696\n",
      "Epoch 16/20\n",
      "200/200 [==============================] - 0s 445us/sample - loss: 0.0742 - accuracy: 1.0000 - val_loss: 0.1202 - val_accuracy: 0.9716\n",
      "Epoch 17/20\n",
      "200/200 [==============================] - 0s 409us/sample - loss: 0.0699 - accuracy: 1.0000 - val_loss: 0.1160 - val_accuracy: 0.9746\n",
      "Epoch 18/20\n",
      "200/200 [==============================] - 0s 468us/sample - loss: 0.0662 - accuracy: 1.0000 - val_loss: 0.1123 - val_accuracy: 0.9746\n",
      "Epoch 19/20\n",
      "200/200 [==============================] - 0s 421us/sample - loss: 0.0629 - accuracy: 1.0000 - val_loss: 0.1090 - val_accuracy: 0.9767\n",
      "Epoch 20/20\n",
      "200/200 [==============================] - 0s 453us/sample - loss: 0.0599 - accuracy: 1.0000 - val_loss: 0.1059 - val_accuracy: 0.9767\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f0a78589438>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_B.compile(loss='binary_crossentropy', optimizer=keras.optimizers.SGD(lr=1e-3), metrics=['accuracy'])\n",
    "model_B.fit(X_train_B, y_train_B, epochs=20, validation_data=(X_valid_B, y_valid_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200 samples, validate on 986 samples\n",
      "Epoch 1/4\n",
      "200/200 [==============================] - 1s 5ms/sample - loss: 0.9664 - accuracy: 0.6500 - val_loss: 0.9030 - val_accuracy: 0.6714\n",
      "Epoch 2/4\n",
      "200/200 [==============================] - 0s 450us/sample - loss: 0.9118 - accuracy: 0.6550 - val_loss: 0.8483 - val_accuracy: 0.6805\n",
      "Epoch 3/4\n",
      "200/200 [==============================] - 0s 454us/sample - loss: 0.8549 - accuracy: 0.6850 - val_loss: 0.7977 - val_accuracy: 0.6957\n",
      "Epoch 4/4\n",
      "200/200 [==============================] - 0s 429us/sample - loss: 0.8028 - accuracy: 0.7000 - val_loss: 0.7543 - val_accuracy: 0.7028\n"
     ]
    }
   ],
   "source": [
    "# clone in order to save the weights\n",
    "model_A_clone = keras.models.clone_model(model_A)\n",
    "model_A_clone.set_weights(model_A.get_weights())\n",
    "\n",
    "model_B_on_A = keras.models.Sequential(model_A.layers[:-1])\n",
    "model_B_on_A.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = False\n",
    "\n",
    "model_B_on_A.compile(loss='binary_crossentropy',\n",
    "                     optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "                     metrics=['accuracy'])\n",
    "\n",
    "# train for a few epochs to adjust the last layer (with random initializations)\n",
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=4, validation_data=(X_valid_B, y_valid_B))\n",
    "\n",
    "# unfreeze the layers, lower the LR and continue training\n",
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = True\n",
    "    \n",
    "optimizer = keras.optimizers.SGD(lr=1e-4)\n",
    "model_B_on_A.compile(loss='binary_crossentropy', optimizer=keras.optimizers.SGD(lr=1e-4), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200 samples, validate on 986 samples\n",
      "Epoch 1/64\n",
      "200/200 [==============================] - 0s 588us/sample - loss: 0.3622 - accuracy: 0.8200 - val_loss: 0.3541 - val_accuracy: 0.8164\n",
      "Epoch 2/64\n",
      "200/200 [==============================] - 0s 469us/sample - loss: 0.3505 - accuracy: 0.8200 - val_loss: 0.3423 - val_accuracy: 0.8245\n",
      "Epoch 3/64\n",
      "200/200 [==============================] - 0s 420us/sample - loss: 0.3382 - accuracy: 0.8200 - val_loss: 0.3329 - val_accuracy: 0.8296\n",
      "Epoch 4/64\n",
      "200/200 [==============================] - 0s 412us/sample - loss: 0.3284 - accuracy: 0.8250 - val_loss: 0.3243 - val_accuracy: 0.8367\n",
      "Epoch 5/64\n",
      "200/200 [==============================] - 0s 407us/sample - loss: 0.3195 - accuracy: 0.8250 - val_loss: 0.3154 - val_accuracy: 0.8387\n",
      "Epoch 6/64\n",
      "200/200 [==============================] - 0s 474us/sample - loss: 0.3102 - accuracy: 0.8250 - val_loss: 0.3074 - val_accuracy: 0.8387\n",
      "Epoch 7/64\n",
      "200/200 [==============================] - 0s 402us/sample - loss: 0.3019 - accuracy: 0.8250 - val_loss: 0.2990 - val_accuracy: 0.8448\n",
      "Epoch 8/64\n",
      "200/200 [==============================] - 0s 484us/sample - loss: 0.2933 - accuracy: 0.8300 - val_loss: 0.2918 - val_accuracy: 0.8499\n",
      "Epoch 9/64\n",
      "200/200 [==============================] - 0s 444us/sample - loss: 0.2858 - accuracy: 0.8300 - val_loss: 0.2848 - val_accuracy: 0.8560\n",
      "Epoch 10/64\n",
      "200/200 [==============================] - 0s 459us/sample - loss: 0.2786 - accuracy: 0.8300 - val_loss: 0.2777 - val_accuracy: 0.8631\n",
      "Epoch 11/64\n",
      "200/200 [==============================] - 0s 546us/sample - loss: 0.2713 - accuracy: 0.8300 - val_loss: 0.2708 - val_accuracy: 0.8692\n",
      "Epoch 12/64\n",
      "200/200 [==============================] - 0s 616us/sample - loss: 0.2642 - accuracy: 0.8400 - val_loss: 0.2647 - val_accuracy: 0.8712\n",
      "Epoch 13/64\n",
      "200/200 [==============================] - 0s 631us/sample - loss: 0.2580 - accuracy: 0.8450 - val_loss: 0.2582 - val_accuracy: 0.8763\n",
      "Epoch 14/64\n",
      "200/200 [==============================] - 0s 431us/sample - loss: 0.2512 - accuracy: 0.8450 - val_loss: 0.2525 - val_accuracy: 0.8834\n",
      "Epoch 15/64\n",
      "200/200 [==============================] - 0s 509us/sample - loss: 0.2454 - accuracy: 0.8500 - val_loss: 0.2467 - val_accuracy: 0.8915\n",
      "Epoch 16/64\n",
      "200/200 [==============================] - 0s 479us/sample - loss: 0.2394 - accuracy: 0.8600 - val_loss: 0.2407 - val_accuracy: 0.8945\n",
      "Epoch 17/64\n",
      "200/200 [==============================] - 0s 434us/sample - loss: 0.2331 - accuracy: 0.8750 - val_loss: 0.2353 - val_accuracy: 0.9026\n",
      "Epoch 18/64\n",
      "200/200 [==============================] - 0s 502us/sample - loss: 0.2276 - accuracy: 0.8850 - val_loss: 0.2303 - val_accuracy: 0.9057\n",
      "Epoch 19/64\n",
      "200/200 [==============================] - 0s 587us/sample - loss: 0.2224 - accuracy: 0.9000 - val_loss: 0.2255 - val_accuracy: 0.9097\n",
      "Epoch 20/64\n",
      "200/200 [==============================] - 0s 535us/sample - loss: 0.2175 - accuracy: 0.9000 - val_loss: 0.2209 - val_accuracy: 0.9148\n",
      "Epoch 21/64\n",
      "200/200 [==============================] - 0s 471us/sample - loss: 0.2127 - accuracy: 0.9000 - val_loss: 0.2164 - val_accuracy: 0.9178\n",
      "Epoch 22/64\n",
      "200/200 [==============================] - 0s 599us/sample - loss: 0.2081 - accuracy: 0.9100 - val_loss: 0.2113 - val_accuracy: 0.9199\n",
      "Epoch 23/64\n",
      "200/200 [==============================] - 0s 450us/sample - loss: 0.2029 - accuracy: 0.9100 - val_loss: 0.2071 - val_accuracy: 0.9229\n",
      "Epoch 24/64\n",
      "200/200 [==============================] - 0s 635us/sample - loss: 0.1986 - accuracy: 0.9150 - val_loss: 0.2031 - val_accuracy: 0.9249\n",
      "Epoch 25/64\n",
      "200/200 [==============================] - 0s 564us/sample - loss: 0.1944 - accuracy: 0.9200 - val_loss: 0.1992 - val_accuracy: 0.9270\n",
      "Epoch 26/64\n",
      "200/200 [==============================] - 0s 460us/sample - loss: 0.1904 - accuracy: 0.9200 - val_loss: 0.1951 - val_accuracy: 0.9300\n",
      "Epoch 27/64\n",
      "200/200 [==============================] - 0s 432us/sample - loss: 0.1863 - accuracy: 0.9300 - val_loss: 0.1916 - val_accuracy: 0.9351\n",
      "Epoch 28/64\n",
      "200/200 [==============================] - 0s 357us/sample - loss: 0.1826 - accuracy: 0.9350 - val_loss: 0.1882 - val_accuracy: 0.9371\n",
      "Epoch 29/64\n",
      "200/200 [==============================] - 0s 434us/sample - loss: 0.1792 - accuracy: 0.9400 - val_loss: 0.1846 - val_accuracy: 0.9381\n",
      "Epoch 30/64\n",
      "200/200 [==============================] - 0s 380us/sample - loss: 0.1755 - accuracy: 0.9400 - val_loss: 0.1815 - val_accuracy: 0.9422\n",
      "Epoch 31/64\n",
      "200/200 [==============================] - 0s 475us/sample - loss: 0.1723 - accuracy: 0.9400 - val_loss: 0.1780 - val_accuracy: 0.9442\n",
      "Epoch 32/64\n",
      "200/200 [==============================] - 0s 490us/sample - loss: 0.1688 - accuracy: 0.9550 - val_loss: 0.1751 - val_accuracy: 0.9473\n",
      "Epoch 33/64\n",
      "200/200 [==============================] - 0s 517us/sample - loss: 0.1658 - accuracy: 0.9550 - val_loss: 0.1721 - val_accuracy: 0.9493\n",
      "Epoch 34/64\n",
      "200/200 [==============================] - 0s 554us/sample - loss: 0.1628 - accuracy: 0.9600 - val_loss: 0.1694 - val_accuracy: 0.9523\n",
      "Epoch 35/64\n",
      "200/200 [==============================] - 0s 439us/sample - loss: 0.1600 - accuracy: 0.9700 - val_loss: 0.1666 - val_accuracy: 0.9574\n",
      "Epoch 36/64\n",
      "200/200 [==============================] - 0s 441us/sample - loss: 0.1571 - accuracy: 0.9700 - val_loss: 0.1639 - val_accuracy: 0.9615\n",
      "Epoch 37/64\n",
      "200/200 [==============================] - 0s 475us/sample - loss: 0.1543 - accuracy: 0.9700 - val_loss: 0.1611 - val_accuracy: 0.9635\n",
      "Epoch 38/64\n",
      "200/200 [==============================] - 0s 435us/sample - loss: 0.1515 - accuracy: 0.9750 - val_loss: 0.1585 - val_accuracy: 0.9635\n",
      "Epoch 39/64\n",
      "200/200 [==============================] - 0s 511us/sample - loss: 0.1488 - accuracy: 0.9800 - val_loss: 0.1559 - val_accuracy: 0.9635\n",
      "Epoch 40/64\n",
      "200/200 [==============================] - 0s 536us/sample - loss: 0.1462 - accuracy: 0.9800 - val_loss: 0.1536 - val_accuracy: 0.9665\n",
      "Epoch 41/64\n",
      "200/200 [==============================] - 0s 515us/sample - loss: 0.1438 - accuracy: 0.9800 - val_loss: 0.1513 - val_accuracy: 0.9675\n",
      "Epoch 42/64\n",
      "200/200 [==============================] - 0s 559us/sample - loss: 0.1415 - accuracy: 0.9800 - val_loss: 0.1491 - val_accuracy: 0.9675\n",
      "Epoch 43/64\n",
      "200/200 [==============================] - 0s 445us/sample - loss: 0.1392 - accuracy: 0.9800 - val_loss: 0.1469 - val_accuracy: 0.9726\n",
      "Epoch 44/64\n",
      "200/200 [==============================] - 0s 482us/sample - loss: 0.1370 - accuracy: 0.9800 - val_loss: 0.1448 - val_accuracy: 0.9736\n",
      "Epoch 45/64\n",
      "200/200 [==============================] - 0s 451us/sample - loss: 0.1348 - accuracy: 0.9800 - val_loss: 0.1428 - val_accuracy: 0.9746\n",
      "Epoch 46/64\n",
      "200/200 [==============================] - 0s 536us/sample - loss: 0.1328 - accuracy: 0.9800 - val_loss: 0.1408 - val_accuracy: 0.9746\n",
      "Epoch 47/64\n",
      "200/200 [==============================] - 0s 601us/sample - loss: 0.1307 - accuracy: 0.9800 - val_loss: 0.1387 - val_accuracy: 0.9746\n",
      "Epoch 48/64\n",
      "200/200 [==============================] - 0s 455us/sample - loss: 0.1286 - accuracy: 0.9850 - val_loss: 0.1370 - val_accuracy: 0.9757\n",
      "Epoch 49/64\n",
      "200/200 [==============================] - 0s 503us/sample - loss: 0.1269 - accuracy: 0.9850 - val_loss: 0.1352 - val_accuracy: 0.9767\n",
      "Epoch 50/64\n",
      "200/200 [==============================] - 0s 503us/sample - loss: 0.1250 - accuracy: 0.9850 - val_loss: 0.1335 - val_accuracy: 0.9787\n",
      "Epoch 51/64\n",
      "200/200 [==============================] - 0s 390us/sample - loss: 0.1233 - accuracy: 0.9850 - val_loss: 0.1317 - val_accuracy: 0.9787\n",
      "Epoch 52/64\n",
      "200/200 [==============================] - 0s 516us/sample - loss: 0.1215 - accuracy: 0.9850 - val_loss: 0.1300 - val_accuracy: 0.9807\n",
      "Epoch 53/64\n",
      "200/200 [==============================] - 0s 589us/sample - loss: 0.1197 - accuracy: 0.9850 - val_loss: 0.1285 - val_accuracy: 0.9807\n",
      "Epoch 54/64\n",
      "200/200 [==============================] - 0s 443us/sample - loss: 0.1182 - accuracy: 0.9850 - val_loss: 0.1266 - val_accuracy: 0.9828\n",
      "Epoch 55/64\n",
      "200/200 [==============================] - 0s 479us/sample - loss: 0.1163 - accuracy: 0.9850 - val_loss: 0.1251 - val_accuracy: 0.9828\n",
      "Epoch 56/64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 1s 3ms/sample - loss: 0.1148 - accuracy: 0.9850 - val_loss: 0.1237 - val_accuracy: 0.9838\n",
      "Epoch 57/64\n",
      "200/200 [==============================] - 0s 506us/sample - loss: 0.1134 - accuracy: 0.9850 - val_loss: 0.1222 - val_accuracy: 0.9848\n",
      "Epoch 58/64\n",
      "200/200 [==============================] - 0s 425us/sample - loss: 0.1118 - accuracy: 0.9850 - val_loss: 0.1208 - val_accuracy: 0.9848\n",
      "Epoch 59/64\n",
      "200/200 [==============================] - 0s 395us/sample - loss: 0.1104 - accuracy: 0.9850 - val_loss: 0.1193 - val_accuracy: 0.9848\n",
      "Epoch 60/64\n",
      "200/200 [==============================] - 0s 530us/sample - loss: 0.1090 - accuracy: 0.9850 - val_loss: 0.1181 - val_accuracy: 0.9868\n",
      "Epoch 61/64\n",
      "200/200 [==============================] - 0s 546us/sample - loss: 0.1076 - accuracy: 0.9850 - val_loss: 0.1168 - val_accuracy: 0.9878\n",
      "Epoch 62/64\n",
      "200/200 [==============================] - 0s 513us/sample - loss: 0.1064 - accuracy: 0.9850 - val_loss: 0.1156 - val_accuracy: 0.9878\n",
      "Epoch 63/64\n",
      "200/200 [==============================] - 0s 469us/sample - loss: 0.1051 - accuracy: 0.9900 - val_loss: 0.1143 - val_accuracy: 0.9888\n",
      "Epoch 64/64\n",
      "200/200 [==============================] - 0s 464us/sample - loss: 0.1038 - accuracy: 0.9900 - val_loss: 0.1131 - val_accuracy: 0.9888\n"
     ]
    }
   ],
   "source": [
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=64, validation_data=(X_valid_B, y_valid_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 0s 104us/sample - loss: 0.6739 - accuracy: 0.7095\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6738625311851502, 0.7095]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_B_on_A.evaluate(X_test_B, y_test_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "RegularizedDense = partial(keras.layers.Dense,activation=\"elu\",\n",
    "                                              kernel_initializer=\"he_normal\",\n",
    "                                              kernel_regularizer=keras.regularizers.l2(0.01))\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    RegularizedDense(300),\n",
    "    RegularizedDense(100),\n",
    "    RegularizedDense(10, activation=\"softmax\", kernel_initializer=\"glorot_uniform\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_6 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
